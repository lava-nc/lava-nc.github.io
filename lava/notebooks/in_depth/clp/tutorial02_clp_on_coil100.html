<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Continually Learning Prototypes (CLP) for Object Learning &mdash; Lava  documentation</title>
      <link rel="stylesheet" href="../../../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../../../_static/graphviz.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="../../../../_static/jquery.js"></script>
        <script src="../../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
        <script src="../../../../_static/doctools.js"></script>
        <script src="../../../../_static/sphinx_highlight.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../../index.html" class="icon icon-home">
            Lava
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../lava_architecture_overview.html">Lava Architecture</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../lava_architecture_overview.html#key-attributes">Key attributes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../lava_architecture_overview.html#why-do-we-need-lava">Why do we need Lava?</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../lava_architecture_overview.html#lava-s-foundational-concepts">Lava’s foundational concepts</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava_architecture_overview.html#processes">1. Processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava_architecture_overview.html#behavioral-implementations-via-processmodels">2. Behavioral implementations via ProcessModels</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava_architecture_overview.html#composability-and-connectivity">3. Composability and connectivity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava_architecture_overview.html#cross-platform-execution">4. Cross-platform execution</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../lava_architecture_overview.html#lava-software-stack">Lava software stack</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../getting_started_with_lava.html">Getting Started with Lava</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorial01_installing_lava.html">Installing Lava</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial01_installing_lava.html#1.-System-Requirements">1. System Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial01_installing_lava.html#2.-Getting-Started">2. Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial01_installing_lava.html#2.1-Cloning-Lava-and-Running-from-Source">2.1 Cloning Lava and Running from Source</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial01_installing_lava.html#2.2-[Alternative]-Installing-Lava-from-Binaries">2.2 [Alternative] Installing Lava from Binaries</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial01_installing_lava.html#3.-Running-Lava-on-Intel-Loihi">3. Running Lava on Intel Loihi</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial01_installing_lava.html#4.-Lava-Developer-Guide">4. Lava Developer Guide</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial01_installing_lava.html#5.-Tutorials">5. Tutorials</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial01_installing_lava.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html">Walk through Lava</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#1.-Usage-of-the-Process-Library">1. Usage of the Process Library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Processes">Processes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Ports-and-connections">Ports and connections</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Variables">Variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Record-internal-Vars-over-time">Record internal Vars over time</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Execution">Execution</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Retrieve-recorded-data">Retrieve recorded data</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Summary">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Learn-more-about">Learn more about</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#2.-Create-a-custom-Process">2. Create a custom Process</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Create-a-new-ProcessModel">Create a new ProcessModel</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Use-the-custom-SpikeGenerator">Use the custom SpikeGenerator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#Execute-and-plot">Execute and plot</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#id1">Summary</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#id2">Learn more about</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial00_tour_through_lava.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial02_processes.html">Processes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial02_processes.html#Recommended-tutorials-before-starting:">Recommended tutorials before starting:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial02_processes.html#What-is-a-Process?">What is a <em>Process</em>?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial02_processes.html#How-to-build-a-Process?">How to build a <em>Process</em>?</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#Overall-architecture">Overall architecture</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#AbstractProcess:-Defining-Vars,-Ports,-and-the-API"><em>AbstractProcess</em>: Defining <em>Vars</em>, <em>Ports</em>, and the API</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#ProcessModel:-Defining-the-behavior-of-a-Process"><em>ProcessModel</em>: Defining the behavior of a <em>Process</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#Instantiating-the-Process">Instantiating the <em>Process</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial02_processes.html#Interacting-with-Processes">Interacting with <em>Processes</em></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#Accessing-Vars">Accessing <em>Vars</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#Using-custom-APIs">Using custom APIs</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#Executing-a-Process">Executing a <em>Process</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial02_processes.html#Update-Vars">Update <em>Vars</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial02_processes.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial03_process_models.html"><em>ProcessModels</em></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial03_process_models.html#Recommended-tutorials-before-starting:">Recommended tutorials before starting:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial03_process_models.html#Create-a-LIF-Process">Create a LIF <em>Process</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial03_process_models.html#Create-a-Python-LeafProcessModel-that-implements-the-LIF-Process">Create a Python <em>LeafProcessModel</em> that implements the LIF <em>Process</em></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial03_process_models.html#Setup">Setup</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial03_process_models.html#Defining-a-PyLifModel-for-LIF">Defining a <em>PyLifModel</em> for LIF</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial03_process_models.html#Compile-and-run-PyLifModel">Compile and run <em>PyLifModel</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial03_process_models.html#Selecting-1-ProcessModel:-More-on-LeafProcessModel-attributes-and-relations">Selecting 1 <em>ProcessModel</em>: More on <em>LeafProcessModel</em> attributes and relations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial03_process_models.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial04_execution.html">Execution</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial04_execution.html#Recommended-tutorials-before-starting:">Recommended tutorials before starting:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial04_execution.html#Configuring-and-starting-execution">Configuring and starting execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial04_execution.html#Run-conditions">Run conditions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial04_execution.html#Run-configurations">Run configurations</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial04_execution.html#Running-multiple-Processes">Running multiple <em>Processes</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial04_execution.html#Pausing,-resuming,-and-stopping-execution">Pausing, resuming, and stopping execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial04_execution.html#Manual-compilation-and-execution">Manual compilation and execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial04_execution.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial05_connect_processes.html">Connect processes</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial05_connect_processes.html#Recommended-tutorials-before-starting:">Recommended tutorials before starting:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial05_connect_processes.html#Building-a-network-of-Processes">Building a network of <em>Processes</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial05_connect_processes.html#Create-a-connection">Create a connection</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial05_connect_processes.html#Possible-connections">Possible connections</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial05_connect_processes.html#There-are-some-things-to-consider-though:">There are some things to consider though:</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial05_connect_processes.html#Connect-multiple-InPorts-from-a-single-OutPort">Connect multiple <em>InPorts</em> from a single <em>OutPort</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial05_connect_processes.html#Connecting-multiple-InPorts-to-a-single-OutPort">Connecting multiple <em>InPorts</em> to a single <em>OutPort</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial05_connect_processes.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial06_hierarchical_processes.html">Hierarchical <em>Processes</em> and <em>SubProcessModels</em></a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Recommended-tutorials-before-starting:">Recommended tutorials before starting:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Create-LIF-and-Dense-Processes-and-ProcessModels">Create LIF and Dense <em>Processes</em> and <em>ProcessModels</em></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Create-a-Dense-connection-Process">Create a Dense connection <em>Process</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Create-a-Python-Dense-connection-ProcessModel-implementing-the-Loihi-Sync-Protocol-and-requiring-a-CPU-compute-resource">Create a Python Dense connection <em>ProcessModel</em> implementing the Loihi Sync Protocol and requiring a CPU compute resource</a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Create-a-LIF-neuron-Process">Create a LIF neuron <em>Process</em></a></li>
<li class="toctree-l4"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Create-a-Python-LIF-neuron-ProcessModel-implementing-the-Loihi-Sync-Protocol-and-requiring-a-CPU-compute-resource">Create a Python LIF neuron <em>ProcessModel</em> implementing the Loihi Sync Protocol and requiring a CPU compute resource</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Create-a-DenseLayer-Hierarchical-Process-that-encompasses-Dense-and-LIF-Process-behavior">Create a DenseLayer Hierarchical <em>Process</em> that encompasses Dense and LIF <em>Process</em> behavior</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Create-a-SubProcessModel-that-implements-the-DenseLayer-Process-using-Dense-and-LIF-child-Processes">Create a <em>SubProcessModel</em> that implements the DenseLayer <em>Process</em> using Dense and LIF child <em>Processes</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Run-the-DenseLayer-Process">Run the DenseLayer <em>Process</em></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#Run-Connected-DenseLayer-Processes">Run Connected DenseLayer <em>Processes</em></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial06_hierarchical_processes.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial07_remote_memory_access.html">Remote Memory Access</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial07_remote_memory_access.html#Recommended-tutorials-before-starting:">Recommended tutorials before starting:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial07_remote_memory_access.html#Create-a-minimal-Process-and-ProcessModel-with-a-RefPort">Create a minimal <em>Process</em> and <em>ProcessModel</em> with a <em>RefPort</em></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial07_remote_memory_access.html#Create-a-Python-Process-Model-implementing-the-Loihi-Sync-Protocol-and-requiring-a-CPU-compute-resource">Create a Python Process Model implementing the Loihi Sync Protocol and requiring a CPU compute resource</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial07_remote_memory_access.html#Run-the-Processes">Run the <em>Processes</em></a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial07_remote_memory_access.html#Implicit-and-explicit-VarPorts">Implicit and explicit VarPorts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial07_remote_memory_access.html#Options-to-connect-RefPorts-and-VarPorts">Options to connect RefPorts and VarPorts</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial07_remote_memory_access.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html">MNIST Digit Classification with Lava</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#This-tutorial-assumes-that-you:">This tutorial assumes that you:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#This-tutorial-gives-a-bird’s-eye-view-of">This tutorial gives a bird’s-eye view of</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#Our-MNIST-Classifier">Our MNIST Classifier</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#General-Imports">General Imports</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#Lava-Processes">Lava Processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#ProcessModels-for-Python-execution">ProcessModels for Python execution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#Connecting-Processes">Connecting Processes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#Execution-and-results">Execution and results</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial01_mnist_digit_classification.html#Follow-the-links-below-for-deep-dive-tutorials-on-the-concepts-in-this-tutorial:">Follow the links below for deep-dive tutorials on the concepts in this tutorial:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html">Excitatory-Inhibitory Neural Network with Lava</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#This-tutorial-assumes-that-you:">This tutorial assumes that you:</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#This-tutorial-gives-a-high-level-view-of">This tutorial gives a high level view of</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#E/I-Network">E/I Network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#General-imports">General imports</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#E/I-Network-Lava-Process">E/I Network Lava Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#ProcessModels-for-Python-execution">ProcessModels for Python execution</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Rate-neurons">Rate neurons</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Defining-the-parameters-for-the-network">Defining the parameters for the network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Execution-and-Results">Execution and Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Visualizing-the-activity">Visualizing the activity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Further-analysis">Further analysis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Controlling-the-network">Controlling the network</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#LIF-Neurons">LIF Neurons</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#id7">Execution and Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#id8">Visualizing the activity</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#id9">Controlling the network</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#DIfferent-recurrent-activation-regimes">DIfferent recurrent activation regimes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Running-a-ProcessModel-bit-accurate-with-Loihi">Running a ProcessModel bit-accurate with Loihi</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Execution-of-bit-accurate-model">Execution of bit accurate model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../end_to_end/tutorial02_excitatory_inhibitory_network.html#Follow-the-links-below-for-deep-dive-tutorials-on-the-concepts-in-this-tutorial:">Follow the links below for deep-dive tutorials on the concepts in this tutorial:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial08_stdp.html">Spike-timing Dependent Plasticity (STDP)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial08_stdp.html#This-tutorial-assumes-that-you:">This tutorial assumes that you:</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial08_stdp.html#STDP-from-Lavas-Process-Library">STDP from Lavas Process Library</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial08_stdp.html#The-plastic-connection-Process">The plastic connection Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial08_stdp.html#Plot-spike-trains">Plot spike trains</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial08_stdp.html#Plot-traces">Plot traces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial08_stdp.html#Plot-STDP-learning-window-and-weight-changes">Plot STDP learning window and weight changes</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../tutorial09_custom_learning_rules.html">Custom Learning Rules</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#This-tutorial-assumes-that-you:">This tutorial assumes that you:</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#2.-Loihi’s-learning-engine">2. Loihi’s learning engine</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Epoch-based-updates">Epoch-based updates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Synaptic-variables">Synaptic variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Learning-rules">Learning rules</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Dependencies">Dependencies</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Scaling-factors">Scaling factors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Factors">Factors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Traces">Traces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Example:-Basic-pair-based-STDP">Example: Basic pair-based STDP</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Instantiating-LearningRule">Instantiating LearningRule</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#The-plastic-connection-Process">The plastic connection Process</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Plot-spike-trains">Plot spike trains</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Plot-traces">Plot traces</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Plot-STDP-learning-window-and-weight-changes">Plot STDP learning window and weight changes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../tutorial09_custom_learning_rules.html#Follow-the-links-below-for-deep-dive-tutorials-on-the-concepts-in-this-tutorial:">Follow the links below for deep-dive tutorials on the concepts in this tutorial:</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html">Three Factor Learning with Lava</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#This-tutorial-assumes-that-you:">This tutorial assumes that you:</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Defining-three-factor-learning-rule-interfaces-in-Lava">Defining three-factor learning rule interfaces in Lava</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Reward-modulated-Spike-Timing-Dependent-Plasticity-(R-STDP)-learning-rule">Reward-modulated Spike-Timing Dependent Plasticity (R-STDP) learning rule</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Defining-a-simple-learning-network-with-localized-reward-signals">Defining a simple learning network with localized reward signals</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Initialize-network-parameters-and-weights">Initialize network parameters and weights</a></li>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Generate-binary-input-and-graded-reward-spikes">Generate binary input and graded reward spikes</a></li>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Initialize-Network-Processes">Initialize Network Processes</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Connect-Network-Processes">Connect Network Processes</a></li>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Create-monitors-to-observe-the-weight-and-trace-dynamics-during-learning">Create monitors to observe the weight and trace dynamics during learning</a></li>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Run-the-network">Run the network</a></li>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Visualize-the-learning-results">Visualize the learning results</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Plot-eligibility-trace-dynamics">Plot eligibility trace dynamics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Plot-reward-trace-dynamics">Plot reward trace dynamics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Advanced-Topic:-Implementing-custom-learning-rule-interfaces">Advanced Topic: Implementing custom learning rule interfaces</a></li>
<li class="toctree-l4"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#How-to-learn-more?">How to learn more?</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html#Follow-the-links-below-for-deep-dive-tutorials-on-the-concepts-in-this-tutorial:">Follow the links below for deep-dive tutorials on the concepts in this tutorial:</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../algorithms.html">Algorithms and Application Libraries</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../dl.html">Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../dl.html#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dl.html#lava-dl-workflow">Lava-DL Workflow</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dl.html#getting-started">Getting Started</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dl.html#slayer-2-0">SLAYER 2.0</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../dl.html#example-code">Example Code</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dl.html#bootstrap">Bootstrap</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../dl.html#example-code-1">Example Code</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dl.html#network-exchange-netx-library">Network Exchange (NetX) Library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../dl.html#example-code-2">Example Code</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dl.html#detailed-description">Detailed Description</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/slayer.html">Lava-DL SLAYER</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/bootstrap/bootstrap.html">Lava-DL Bootstrap</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/netx/netx.html">Lava-DL NetX</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../dnf.html">Dynamic Neural Fields</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../dnf.html#introduction">Introduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dnf.html#what-is-lava-dnf">What is lava-dnf?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dnf.html#key-features">Key features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../dnf.html#example">Example</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../optimization.html">Neuromorphic Constrained Optimization Library</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../optimization.html#about-the-project">About the Project</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#taxonomy-of-optimization-problems">Taxonomy of Optimization Problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#optimizationsolver-and-optimizationproblem-classes">OptimizationSolver and OptimizationProblem Classes</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../optimization.html#tutorials">Tutorials</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#quadratic-programming">Quadratic Programming</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#quadratic-uncosntrained-binary-optimization">Quadratic Uncosntrained Binary Optimization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../optimization.html#examples">Examples</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#solving-qp-problems">Solving QP problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#solving-qubo">Solving QUBO</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../optimization.html#getting-started">Getting Started</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#requirements">Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#installation">Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../optimization.html#alternative-installing-lava-via-conda">[Alternative] Installing Lava via Conda</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../developer_guide.html">Developer Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#lava-s-origins">Lava’s Origins</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#contact-information">Contact Information</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#development-roadmap">Development Roadmap</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#initial-release">Initial Release</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#how-to-contribute-to-lava">How to contribute to Lava</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#open-an-issue">Open an Issue</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#pull-request-checklist">Pull Request Checklist</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#open-a-pull-request">Open a Pull Request</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#coding-conventions">Coding Conventions</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#code-requirements">Code Requirements</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#guidelines">Guidelines</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#docstring-format">Docstring Format</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#contributors">Contributors</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#contributor">Contributor</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#committer">Committer</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../developer_guide.html#list-of-lava-nc-lava-project-committers">List of lava-nc/lava Project Committers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../developer_guide.html#list-of-lava-nc-lava-dnf-project-committers">List of lava-nc/lava-dnf Project Committers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../developer_guide.html#list-of-lava-nc-lava-optimization-project-committers">List of lava-nc/lava-optimization Project Committers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../developer_guide.html#list-of-lava-nc-lava-dl-project-committers">List of lava-nc/lava-dl Project Committers</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../developer_guide.html#committer-promotion">Committer Promotion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#repository-structure">Repository Structure</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#id17">lava-nc/lava</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#lava-nc-lava-dnf">lava-nc/lava-dnf</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#lava-nc-lava-dl">lava-nc/lava-dl</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#lava-nc-lava-optimization">lava-nc/lava-optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../developer_guide.html#lava-nc-lava-docs">lava-nc/lava-docs</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#code-of-conduct">Code of Conduct</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../developer_guide.html#licenses">Licenses</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../../lava_api_documentation.html">Lava API Documentation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../lava.html">Lava</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../lava.magma.html">Magma</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.magma.compiler.html">lava.magma.compiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.magma.core.html">lava.magma.core</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.magma.runtime.html">lava.magma.runtime</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../lava.proc.html">Lava process library</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.conv.html">lava.proc.conv</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.dense.html">lava.proc.dense</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.io.html">lava.proc.io</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.learning_rules.html">lava.proc.learning_rules</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.lif.html">lava.proc.lif</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.monitor.html">lava.proc.monitor</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.receiver.html">lava.proc.receiver</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.sdn.html">lava.proc.sdn</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.proc.spiker.html">lava.proc.spiker</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../lava.utils.html">Lava Utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.utils.dataloader.html">lava.utils.dataloader</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.utils.html#lava-utils-float2fixed">lava.utils.float2fixed</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.utils.html#lava-utils-profiler">lava.utils.profiler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.utils.html#lava-utils-system">lava.utils.system</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.utils.html#lava-utils-validator">lava.utils.validator</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.utils.html#lava-utils-visualizer">lava.utils.visualizer</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../lava.utils.html#lava-utils-weightutils">lava.utils.weightutils</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../lava-lib-dl/index.html">Lava - Deep Learning</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dl/slayer/index.html">SLAYER</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/neuron/modules.html">Neuron</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/synapse/modules.html">Synapse</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/spike/modules.html">Spike</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/axon/modules.html">Axon</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/dendrite/modules.html">Dendrite</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/block/modules.html">Blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/loss.html">Loss</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/classifier.html">Classifier</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/io.html">Input/Output</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/auto.html">Auto</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/slayer/utils/modules.html">Utilities</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dl/slayer/index.html#indices-and-tables">Indices and tables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dl/bootstrap/index.html">Bootstrap (ANN-SNN training)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/bootstrap/block/modules.html">Blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/bootstrap/ann_sampler.html">ANN Statistics Sampler</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/bootstrap/routine.html">Routine</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dl/bootstrap/index.html#indices-and-tables">Indices and tables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dl/netx/index.html">Lava-DL NetX</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/netx/blocks/modules.html">Blocks</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/netx/hdf5.html">HDF5</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dl/netx/utils.html">Utils</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dl/netx/index.html#indices-and-tables">Indices and tables</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.html">Lava - Dynamic Neural Fields</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.connect.html">lava.lib.dnf.connect</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.connect.html#lava-lib-dnf-connect-connect">lava.lib.dnf.connect.connect</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.connect.html#lava-lib-dnf-connect-exceptions">lava.lib.dnf.connect.exceptions</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.kernels.html">lava.lib.dnf.kernels</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.kernels.html#lava-lib-dnf-kernels-kernels">lava.lib.dnf.kernels.kernels</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.operations.html">lava.lib.dnf.operations</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.operations.html#lava-lib-dnf-operations-enums">lava.lib.dnf.operations.enums</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.operations.html#lava-lib-dnf-operations-exceptions">lava.lib.dnf.operations.exceptions</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.operations.html#lava-lib-dnf-operations-operations">lava.lib.dnf.operations.operations</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.operations.html#lava-lib-dnf-operations-shape-handlers">lava.lib.dnf.operations.shape_handlers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.inputs.html">lava.lib.dnf.inputs</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.inputs.gauss_pattern.html">lava.lib.dnf.inputs.gauss_pattern</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.inputs.rate_code_spike_gen.html">lava.lib.dnf.inputs.rate_code_spike_gen</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.utils.html">lava.lib.dnf.utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.utils.html#lava-lib-dnf-utils-convenience">lava.lib.dnf.utils.convenience</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.utils.html#lava-lib-dnf-utils-math">lava.lib.dnf.utils.math</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.utils.html#lava-lib-dnf-utils-plotting">lava.lib.dnf.utils.plotting</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-dnf/lava.lib.dnf.utils.html#lava-lib-dnf-utils-validation">lava.lib.dnf.utils.validation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.html">Lava - Optimization</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.problems.html">lava.lib.optimization.problems</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.problems.bayesian.html">lava.lib.optimization.problems.bayesian</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.problems.html#lava-lib-optimization-problems-coefficients">lava.lib.optimization.problems.coefficients</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.problems.html#lava-lib-optimization-problems-constraints">lava.lib.optimization.problems.constraints</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.problems.html#lava-lib-optimization-problems-cost">lava.lib.optimization.problems.cost</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.problems.html#lava-lib-optimization-problems-problems">lava.lib.optimization.problems.problems</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.problems.html#lava-lib-optimization-problems-variables">lava.lib.optimization.problems.variables</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.solvers.html">lava.lib.optimization.solvers</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.solvers.bayesian.html">lava.lib.optimization.solvers.bayesian</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.solvers.generic.html">lava.lib.optimization.solvers.generic</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.solvers.qp.html">lava.lib.optimization.solvers.qp</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.utils.html">lava.lib.optimization.utils</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.utils.generators.html">lava.lib.optimization.utils.generators</a></li>
<li class="toctree-l4"><a class="reference internal" href="../../../../lava-lib-optimization/lava.lib.optimization.utils.html#lava-lib-optimization-utils-solver-tuner">lava.lib.optimization.utils.solver_tuner</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../../index.html">Lava</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Continually Learning Prototypes (CLP) for Object Learning</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../../_sources/lava/notebooks/in_depth/clp/tutorial02_clp_on_coil100.ipynb.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars and line breaks on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
    white-space: pre;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
.jp-RenderedHTMLCommon table,
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
.jp-RenderedHTMLCommon thead,
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
.jp-RenderedHTMLCommon tr,
.jp-RenderedHTMLCommon th,
.jp-RenderedHTMLCommon td,
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
.jp-RenderedHTMLCommon th,
div.rendered_html th {
  font-weight: bold;
}
.jp-RenderedHTMLCommon tbody tr:nth-child(odd),
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
.jp-RenderedHTMLCommon tbody tr:hover,
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<p><em>Copyright (C) 2023 Intel Corporation</em> <em>SPDX-License-Identifier: BSD-3-Clause</em> <em>See: https://spdx.org/licenses/</em></p>
<hr class="docutils" />
<section id="Continually-Learning-Prototypes-(CLP)-for-Object-Learning">
<h1>Continually Learning Prototypes (CLP) for Object Learning<a class="headerlink" href="#Continually-Learning-Prototypes-(CLP)-for-Object-Learning" title="Permalink to this heading"></a></h1>
<section id="Motivation:">
<h2>Motivation:<a class="headerlink" href="#Motivation:" title="Permalink to this heading"></a></h2>
<p>This tutorial demonstrates the continual learning of objects on real data with our CLP algorithm.</p>
</section>
<section id="Introduction:">
<h2>Introduction:<a class="headerlink" href="#Introduction:" title="Permalink to this heading"></a></h2>
<p>In the context of online continual learning, the approach we refer to as “Continually Learning Prototypes” is an evolving paradigm combining supervised, unsupervised, and semi-supervised learning elements. It begins with a supervised continual learning phase, where the base classes are learned from labeled data. However, it doesn’t stop there. This approach seamlessly integrates open-set learning to enhance adaptability and robustness. Overall, it is engineered to meet the evolving challenges of
dynamic environments where data may encompass familiar and novel classes.</p>
<p><strong>Novelty Detection:</strong> One of the core contributions of this approach lies in its novelty detection mechanism, executed through prototype neurons. We implement an open-set learning mechanism by incorporating a threshold mechanism for the prototype neurons. When faced with data from unfamiliar categories, the model detects them as unknown instances. This capability prevents the model from making misguided predictions on novel data. The instance that triggers novelty detection becomes the focal
point for further processing.</p>
<p><strong>Learning Novel Clusters:</strong> The instance that initially triggered the novelty detection serves as the cluster center, capturing the essence of the potentially novel categories or a novel subclass of a known class. The novelty detection mechanism ensures that these clusters are significant departures from known classes.</p>
<p><strong>User Labeling for Novel Clusters:</strong> The novel clusters identified via unsupervised clustering are labeled through user interactions. A user provides labels for representative instances within each cluster, allowing the model to understand and classify these newly discovered categories. This user-in-the-loop process gradually enriches the model’s knowledge over time.</p>
<p><strong>Few-shot learning:</strong> Thanks to the prototype-based nature of this approach, the model can learn from new or rarely seen categories based on a limited number of examples.</p>
<p><strong>Adaptation to Dynamic Environments:</strong> This approach is well-suited for dynamic and open-world environments where new classes can emerge unpredictably. Its strength lies in its adaptability and continual learning capacity, ensuring that the model remains relevant and accurate as the environment evolves.</p>
<p>In summary, “Continually Learning Prototypes” combines continual learning, open-set recognition, and user involvement to create a flexible and adaptive learning system. It effectively handles both known and unknown data, making it a valuable asset in applications where the data landscape is ever-changing.</p>
<p>The CLP is based on our previous work on <a class="reference external" href="https://dl.acm.org/doi/abs/10.1145/3546790.3546791">Interactive continual learning for robots: a neuromorphic approach</a> wiht Loihi 1.</p>
</section>
<section id="This-tutorial-assumes-that-you:">
<h2>This tutorial assumes that you:<a class="headerlink" href="#This-tutorial-assumes-that-you:" title="Permalink to this heading"></a></h2>
<ul class="simple">
<li><p>have the <a class="reference internal" href="../tutorial01_installing_lava.html"><span class="doc">Lava framework installed</span></a></p></li>
<li><p>are familiar with <a class="reference internal" href="../tutorial02_processes.html"><span class="doc">Process interfaces in Lava</span></a></p></li>
<li><p>are familiar with <a class="reference external" href="../../in_depth/tutorial02_process_models.ipynb">ProcessModel implementations in Lava</a></p></li>
<li><p>are familiar with how to <a class="reference internal" href="../tutorial05_connect_processes.html"><span class="doc">connect Lava Processes</span></a></p></li>
<li><p>are familiar with how to <a class="reference internal" href="../tutorial08_stdp.html"><span class="doc">implement a custom learning rule</span></a></p></li>
<li><p>are familiar with how to <a class="reference internal" href="../three_factor_learning/tutorial01_Reward_Modulated_STDP.html"><span class="doc">implement a reward-modulated learning rule</span></a></p></li>
<li><p>are familiar with <a class="reference internal" href="tutorial01_one-shot_learning_with_novelty_detection.html"><span class="doc">intorductory tutorial on CLP</span></a></p></li>
</ul>
</section>
<section id="Overview">
<h2>Overview<a class="headerlink" href="#Overview" title="Permalink to this heading"></a></h2>
<p>In this tutorial, we will extend experiments with CLP to a real dataset. We will still stick to the online continual learning (OCL) setting, where we aim to learn new classes from a stream of non-i.i.d. data, one sample at a time, while not forgetting old classes. Compared to the introductory CLP tutorial, we will use an upgraded version of the algorithm here. Specifically, this version is concurrently capable of both unsupervised and supervised continual learning. We will perform the
experiments both in multi-shot and few-shot settings. Crucially, novelty detection will support open-world recognition, where known classes are classified and unknown classes are detected and learned with novelty-triggered one-shot learning. While this constitutes unsupervised continual learning, we also allow the error signal from the user to trigger one-shot learning of the pattern as a type of supervised continual learning. The CLP will learn from the extracted features of a dataset
(COIL-100), and we will compare our continual learning results to offline K-NN performance.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
import matplotlib.pylab as plt

from lava.magma.core.run_configs import Loihi2SimCfg
from lava.magma.core.run_conditions import RunSteps
from lava.proc.dense.process import LearningDense
from lava.proc.dense.models import PyLearningDenseModelBitApproximate
</pre></div>
</div>
</div>
<section id="Dasaset:-COIL-100">
<h3>Dasaset: COIL-100<a class="headerlink" href="#Dasaset:-COIL-100" title="Permalink to this heading"></a></h3>
<center><p><img alt="90c4cddf2c23406c8d2b1b8671fa9cb1" src="https://raw.githubusercontent.com/lava-nc/lava-nc.github.io/main/_static/images/tutorial_clp/coil_100.png" /></p>
<center><center><figcaption align="center"><p>Figure 1. COIL-100 dataset. The dataset includes 72 frames for each of the 100 objects rotated on a turntable.</p>
</figcaption><center></section>
<section id="Data-preparation-and-visualization">
<h3>Data preparation and visualization<a class="headerlink" href="#Data-preparation-and-visualization" title="Permalink to this heading"></a></h3>
<section id="Feature-extraction">
<h4>Feature extraction<a class="headerlink" href="#Feature-extraction" title="Permalink to this heading"></a></h4>
<p>CLP works on extracted features, so we need to use some feature extractor to process the raw images first. We have a util function <code class="docutils literal notranslate"><span class="pre">extract_and_load_features</span></code> that uses the ImageNet-trained EfficientNet backbone for this purpose. Note that you need to install <code class="docutils literal notranslate"><span class="pre">torch</span></code> and <code class="docutils literal notranslate"><span class="pre">torchvision</span></code> libraries to run feature extraction. Additionally, you need to download the COIL-100 dataset into this tutorial’s <code class="docutils literal notranslate"><span class="pre">dataset</span></code> folder. You can replace the EfficientNet backbone with any other backbone inside
this helper function.</p>
<p>Alternatively, you can set <code class="docutils literal notranslate"><span class="pre">extract=False</span></code> to use load pre-extracted features from a file included with this tutorial, bypassing all the above requirements. This file contains extracted features from all objects of the COIL-100 datasets.</p>
<p>The extracted features are put through a few smaller preprocessing steps, e.g., rectification (see the comment in the code below for details).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from utils import extract_and_load_features
# Load COIL-100 data and extract features
X, y = extract_and_load_features(extract=False)

# The label==0 is reserved for &quot;no information&quot; on the communication channel
y = y + 1

# The negative feature values are clipped to zero. We have validated that this does not degrade accuracy.
X[X&lt;0]=0

# Normalize the vectors
X = X / np.expand_dims(np.linalg.norm(X, axis=1), axis=1)

# The fraction of the fixed point representation used to translate floating point input to graded spikes
b_fraction = 7
X_fixed = np.round(X * 2 ** b_fraction).astype(np.int32)

# Zero valued neuron is upgraded to &#39;1&#39; so that they can overwrite the x1 trace
X_fixed = X_fixed + 1

print(X.shape, y.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
LIBS_ARE_AVAILABLE:  True
(7200, 1280) (7200,)
</pre></div></div>
</div>
</section>
<section id="Visualize-the-similarities">
<h4>Visualize the similarities<a class="headerlink" href="#Visualize-the-similarities" title="Permalink to this heading"></a></h4>
<p>Let’s visualize a histogram of the similarities of a single random example to all the other samples. Note that we use dot product similarity in the Lava implementation of the CLP algorithm. Those samples from the same class will have high similarity, while the rest (majority) will have lower similarity. We can see that in the histogram below: each color is one random sample, and only a small number of samples has <span class="math">&gt;12000</span> similarity to one of these three samples. This histogram also helps
us roughly know which range of the similarity values (input currents to PrototypeLIF) we should sample with spike times to compete. Namely, we probably need to better sample the range <code class="docutils literal notranslate"><span class="pre">[12000,</span> <span class="pre">18000]</span></code> in terms of spike times.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>sim_single = np.dot(X_fixed[np.random.randint(low=0, high=42*72, size=3),:], X_fixed.T)
plt.figure(figsize=(12,6))
plt.hist(sim_single.T, bins=40)
plt.legend([&quot;Random sample 1&quot;, &quot;Random sample 2&quot;, &quot;Random sample 3&quot;])
plt.title(&quot;Histogram of the similarities of three random samples to all other samples&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_7_0.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_7_0.png" />
</div>
</div>
</section>
<section id="Train-test-split">
<h4>Train-test split<a class="headerlink" href="#Train-test-split" title="Permalink to this heading"></a></h4>
<p>There are 72 frames for each object. Let’s go ahead and decide how many samples per class we want to provide for training. If we wish, we can dataset by the factor <code class="docutils literal notranslate"><span class="pre">k_sample</span></code>, as the consecutive image frames are very similar. In this case, we will sample by a factor of 2 (36 frames per object). We will reserve ten frames for training and 26 for testing for our first experiment, i.e., the multi-shot supervised CL.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.model_selection import train_test_split

# Subsample the dataset
k_sample = 2
X_sampled = X_fixed[::k_sample, :]
y_sampled = y[::k_sample]

test_size = 26/36
X_train, X_test, y_train, y_test = train_test_split(X_sampled, y_sampled, test_size=test_size, random_state=1000, stratify=y_sampled)
</pre></div>
</div>
</div>
</section>
<section id="Ordering-of-the-data:-non-i.i.d.-stream-learning">
<h4>Ordering of the data: non-i.i.d. stream learning<a class="headerlink" href="#Ordering-of-the-data:-non-i.i.d.-stream-learning" title="Permalink to this heading"></a></h4>
<p>Next, we will order the training and testing samples by the objects. This way, we simulate online sequential learning. Note that this data order is non-i.i.d, as there is no shuffling among the objects. However, each object’s frames are shuffled separately in the previous cell.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X_train = X_train[np.argsort(y_train),:]
y_train = y_train[np.argsort(y_train)]
X_test = X_test[np.argsort(y_test),:]
y_test = y_test[np.argsort(y_test)]
</pre></div>
</div>
</div>
<p>Additionally, we can choose how many objects we want to learn out of 100 objects available in the dataset. We have chosen 20 objects for initial training as the full dataset takes a long to run in CPU simulation. But if the simulation takes a long time, you can decrease it further.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>cls_rng = [0, 20]
n_classes = cls_rng[1]

X_train = X_train[(y_train&gt;cls_rng[0]) &amp; (y_train&lt;=cls_rng[1]), :]
X_test = X_test[(y_test&gt;cls_rng[0]) &amp; (y_test&lt;=cls_rng[1]), :]
y_train = y_train[(y_train&gt;cls_rng[0]) &amp; (y_train&lt;=cls_rng[1])]
y_test = y_test[(y_test&gt;cls_rng[0]) &amp; (y_test&lt;=cls_rng[1])]

print(X_train.shape)
print(X_test.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(200, 1280)
(520, 1280)
</pre></div></div>
</div>
</section>
<section id="Visualization-of-the-similarity-between-the-training-and-testing-samples">
<h4>Visualization of the similarity between the training and testing samples<a class="headerlink" href="#Visualization-of-the-similarity-between-the-training-and-testing-samples" title="Permalink to this heading"></a></h4>
<p>Below, we visualize the similarity between training samples (rows) and testing samples (columns). Dark red diagonal entries point to small intra-class variance, while the lighter entries along the diagonal mean those classes have higher intra-class variability. We also observe bright off-diagonal entries, possibly highlighting high similarity/confusion between the corresponding objects.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from matplotlib import colors

a_in = np.arange(10000,19000, 1000)

sims = np.dot(X_train, X_test.T)

# define the colormap
cmap = plt.cm.jet

# extract all colors from the .jet map
cmaplist = [cmap(i) for i in range(cmap.N)]

bounds=a_in
norm = colors.BoundaryNorm(bounds, cmap.N)

plt.figure(figsize=(30,5))
img = plt.imshow(sims, interpolation=&#39;nearest&#39;,cmap=cmap, norm=norm)
plt.colorbar(img, cmap=cmap, norm=norm, boundaries=bounds, ticks=a_in)
plt.title(&quot;Dot product similarity between normalized, 7-bit fixed-point vectors&quot;)
plt.xlabel(&quot;Testing Samples&quot;)
plt.ylabel(&quot;Training Samples&quot;)
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_15_0.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_15_0.png" />
</div>
</div>
</section>
</section>
<section id="Lava-diagram-of-CLP-for-COIL-100-experiment">
<h3>Lava diagram of CLP for COIL-100 experiment<a class="headerlink" href="#Lava-diagram-of-CLP-for-COIL-100-experiment" title="Permalink to this heading"></a></h3>
<center><p><img alt="83617666c8ff4cdb895cb7ccdc67064a" src="https://raw.githubusercontent.com/lava-nc/lava-nc.github.io/main/_static/images/tutorial_clp/clp_coil100_lava_diagram.png" /></p>
<center><center><figcaption align="center"><p>Figure 1. Lava process diagram of the CLP that is capable of both unsupervised and supervised learning</p>
</figcaption><center><p>Compared to the introductory CLP tutorial, we are using an upgraded version of the algorithm here. Specifically, this version is concurrently capable of both unsupervised and supervised learning. This is possible thanks to error feedback from the <code class="docutils literal notranslate"><span class="pre">Readout</span></code> process to the <code class="docutils literal notranslate"><span class="pre">Allocator</span></code> process. Basically, when there is a mismatch between the predicted label and the user-provided true label, the <code class="docutils literal notranslate"><span class="pre">Readout</span></code> process generates an error signal, which triggers the allocation of a new prototype
neuron for this mistaken pattern. As we will see later, this will improve the performance. This error feedback can be turned on or off, and based on this the CLP performs either supervised or unsupervised learning.</p>
</section>
<section id="Temporal-Winner-take-all">
<h3>Temporal Winner-take-all<a class="headerlink" href="#Temporal-Winner-take-all" title="Permalink to this heading"></a></h3>
<p>To facilitate a somewhat precise competition between similar prototypes, we need a temporal winner-take-all mechanism, where the best matching prototype neuron should spike first and inhibit others by sending them a hard-reset signal. This is also shown in the above diagram. However, we need to tune the voltage threshold, current and voltage decay constants of these neurons so that similar prototypes do not spike at the same time step but have at least one time step difference. For this purpose
we want some input current ranges to trigger output spikes at different time steps.</p>
</section>
</section>
<section id="(Optional)-Hyperparameter-search-for-PrototypeLIF">
<h2>(Optional) Hyperparameter search for PrototypeLIF<a class="headerlink" href="#(Optional)-Hyperparameter-search-for-PrototypeLIF" title="Permalink to this heading"></a></h2>
<p>To achieve this, we can do a hyperparameter search on <code class="docutils literal notranslate"><span class="pre">du,</span> <span class="pre">dv,</span> <span class="pre">vth</span></code> so that each input current value from the list of <code class="docutils literal notranslate"><span class="pre">[12000,</span> <span class="pre">13200,</span> <span class="pre">14400,</span> <span class="pre">15600,</span> <span class="pre">16800,</span> <span class="pre">18000]</span></code> elicit output spike at a unique time step. Note that we need a minimum current value that does not elicit a spike and it should be a little smaller than the minimum value in the range above. As this hyperparameter search takes time, we have commented it out, but one can rerun this search for other input current values.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[67]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># from utils import wta_hyperparam_search
# a_in = [11900]+list(range(12000,18800, 1200))
# print(a_in)
# n_steps = 25
# best_params = wta_hyperparam_search(a_in, n_steps)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
-----------------------------
du = 10
dv = 760
vth = 61000
The Latest spike time: 18
-----------------------------
du = 30
dv = 600
vth = 70000
The Latest spike time: 16
-----------------------------
du = 50
dv = 620
vth = 64000
The Latest spike time: 14
-----------------------------
du = 70
dv = 740
vth = 53000
The Latest spike time: 12
Spike time for neuron 1 with a_in=11900: 0
Spike time for neuron 2 with a_in=12000: 12
Spike time for neuron 3 with a_in=13200: 7
Spike time for neuron 4 with a_in=14400: 6
Spike time for neuron 5 with a_in=15600: 5
Spike time for neuron 6 with a_in=16800: 4
Spike time for neuron 7 with a_in=18000: 3
Best Parameter Combination:
du = 70
dv = 740
vth = 53000
</pre></div></div>
</div>
</section>
<section id="Visualize-voltage-dynamics">
<h2>Visualize voltage dynamics<a class="headerlink" href="#Visualize-voltage-dynamics" title="Permalink to this heading"></a></h2>
<p>Let’s also visualize the voltage dynamics with the chosen hyperparameters for the input values we wanted to distinguish in the temporal domain.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from utils import plot_wta_voltage_dynamics
n_steps = 25

# Input values to differentiate based on spike time
a_in = [11900]+list(range(12000,18800, 1200))

# Optimized hyperparameters
du = 70
dv = 740
vth = 53000

plot_wta_voltage_dynamics(du, dv, vth, a_in, n_steps)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_22_0.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_22_0.png" />
</div>
</div>
<section id="Offline-Supervised-Baselines:-K-NN-accuracy">
<h3>Offline Supervised Baselines: K-NN accuracy<a class="headerlink" href="#Offline-Supervised-Baselines:-K-NN-accuracy" title="Permalink to this heading"></a></h3>
<p>Before going into the CLP experiments, let’s find a baseline accuracy. For this purpose, we used k-NN (k=1), the simplest and one of the most well-known prototype-based approaches. Below, we report the offline K-NN (k=1) accuracy is 97.6%. Note that this accuracy can be further improved by using more sophisticated algorithms, but as the dataset is relatively simple, the accuracy is already quite high. Another rationale behind using K-NN is that it is a brute-force and simple version of the
current CLP algorithm, as K-NN stores all data, while CLP stores only those that are novel or have triggered a prediction error. We also chose <code class="docutils literal notranslate"><span class="pre">n=1</span></code> because CLP uses a single winner-take-all mechanism, and using <code class="docutils literal notranslate"><span class="pre">n=1</span></code> for K-NN has a similar effect that the prediction is based on the most similar pattern stored in memory.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix

knn = KNeighborsClassifier(n_neighbors=1, metric=&#39;cosine&#39;)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
acc = accuracy_score(y_test, y_pred)
print(&quot;Offline KNN accuracy (k=1):&quot;, acc.round(3))

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,6))
plt.xticks(np.arange(cls_rng[1]))
plt.yticks(np.arange(cls_rng[1]))
plt.imshow(cm)
plt.colorbar()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Offline KNN accuracy (k=1): 0.971
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_24_1.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_24_1.png" />
</div>
</div>
</section>
<section id="Setup-the-CLP-network">
<h3>Setup the CLP network<a class="headerlink" href="#Setup-the-CLP-network" title="Permalink to this heading"></a></h3>
<p>Now, let’s set up our CLP network.</p>
<p>We initialize the CLP object with the global and process-specific parameters. Specifically, <code class="docutils literal notranslate"><span class="pre">supervised</span></code> is a flag allowing allocation of new prototypes based on the supervision signal of the user (supervised learning), while <code class="docutils literal notranslate"><span class="pre">learn_novels</span></code> allows allocation based on novelty detection (unsupervised). If both are set to false, then the system is in pure inference mode with no learning.</p>
<p><code class="docutils literal notranslate"><span class="pre">n_protos</span></code> is the number of the prototypes, <code class="docutils literal notranslate"><span class="pre">n_features</span></code> is the length of the input feature vector, <code class="docutils literal notranslate"><span class="pre">n_steps_per_sample</span></code> is the number of steps needed to process each input sample, <code class="docutils literal notranslate"><span class="pre">b_fraction</span></code> is the number of the bits for fixed representation of the input.</p>
<p>In addition, <code class="docutils literal notranslate"><span class="pre">du</span></code>, <code class="docutils literal notranslate"><span class="pre">dv</span></code>, <code class="docutils literal notranslate"><span class="pre">vth</span></code> are <code class="docutils literal notranslate"><span class="pre">PrototypeLIF</span></code> parameters that are taken from the above WTA hyperparameter optimization, while <code class="docutils literal notranslate"><span class="pre">t_wait</span></code> is the <code class="docutils literal notranslate"><span class="pre">NoveltyDetector</span></code> parameter that specifies the number of time steps to wait for a PrototypeLIF before declaring the input pattern a novel one. It is set based on the result of the hyperparameter optimization (see above), i.e., the spike time of the latest spike possible after input injection.</p>
<p>Finally, setting the <code class="docutils literal notranslate"><span class="pre">debug</span></code> parameter runs the CLP in debug mode, where current and voltage values are recorded. Here, we are not using this functionality.</p>
<p>We also define run configuration here, which will be shared throughout our experiments.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># General params
n_features = X_train.shape[1]
t_wait = 13  # the waiting window for novelty detection after input injection
n_protos = 100  # number of the prototypes
# The time difference between two consecutive inputs
n_steps_per_sample = t_wait + 8  # To give enough time for dynamics to converge

# PrototypeLIF neural dynamics parameters
du = 70
dv = 740
vth = 53000
exception_map = {
        LearningDense: PyLearningDenseModelBitApproximate
}

run_cfg = Loihi2SimCfg(select_tag=&quot;fixed_pt&quot;,
                       exception_proc_model_map=exception_map)
</pre></div>
</div>
</div>
</section>
<section id="Experiment-1:-Multi-shot-supervised-Online-Continual-Learning-(OCL)">
<h3>Experiment 1: Multi-shot supervised Online Continual Learning (OCL)<a class="headerlink" href="#Experiment-1:-Multi-shot-supervised-Online-Continual-Learning-(OCL)" title="Permalink to this heading"></a></h3>
<p>In the multi-shot supervised learning experiment, we enable both the novelty-triggered and error signal-trigger allocation. We provide ten samples (multi-shot) per object for training and each training sample is provided with the label. As a result, If a novelty is detected, it triggers an allocation and the new prototype is labelled with the provided label. On the other hand, if there is a prediction and the predicted label does not match the true label, an error signal is generated to trigger
the allocation of a new prototype with the correct label. If the prediction matches the actual label, we take no action.</p>
<section id="Initialize-the-CLP-network">
<h4>Initialize the CLP network<a class="headerlink" href="#Initialize-the-CLP-network" title="Permalink to this heading"></a></h4>
<p>After initializing parameters in the previous step, using them, we create an instance of the CLP class, which is a wrapper around processes, connections and helper functions of the entire CLP system. Next, we run the <code class="docutils literal notranslate"><span class="pre">generate_input_spikes()</span></code> function to generate input spikes for the whole run: both input pattern spikes and label input spikes. <code class="docutils literal notranslate"><span class="pre">setup_procs_and_conns()</span></code> function initializes all the necessary processes of the CLP and connects them. In addition, the <code class="docutils literal notranslate"><span class="pre">get_results()</span></code> function
reads the collected results from the monitors. These are the predictions and spike times for novelty, prototype and error spikes. If the <code class="docutils literal notranslate"><span class="pre">debug</span></code> mode is on, you can also get prototype neurons’ current and voltage values.</p>
</section>
<section id="Training-phase">
<h4>Training phase<a class="headerlink" href="#Training-phase" title="Permalink to this heading"></a></h4>
<p>For the training phase of this experiment we set both parameter <code class="docutils literal notranslate"><span class="pre">supervised</span></code> and <code class="docutils literal notranslate"><span class="pre">learn_novels</span></code> true.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from clp import CLP

clp = CLP(supervised=True,
          learn_novels=True,
          n_protos=n_protos,
          n_features=n_features,
          n_steps_per_sample=n_steps_per_sample,
          b_fraction=b_fraction,
          du=du,
          dv=dv,
          vth=vth,
          t_wait=t_wait,
          debug=False)

s_pattern_inp, s_user_label = clp.generate_input_spikes(X_train, y_train)
clp = clp.setup_procs_and_conns()

num_steps = clp.num_steps
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Run

run_cond = RunSteps(num_steps=num_steps)

clp.prototypes.run(condition=run_cond, run_cfg=run_cfg)

novelty_spikes, proto_spikes, error_spikes, preds, currs = clp.get_results()
<br/></pre></div>
</div>
</div>
<p>Let’s visualize output spikes for the novelty, error and the first 10 prototype neurons and the output of the readout process as our main prediction.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from utils import  plot_spikes, plot_spikes_time_series
time = np.arange(num_steps)
plot_spikes_time_series(
        spikes=[np.where(proto_spikes[:, 9])[0],
                np.where(proto_spikes[:, 8])[0],
                np.where(proto_spikes[:, 7])[0],
                np.where(proto_spikes[:, 6])[0],
                np.where(proto_spikes[:, 5])[0],
                np.where(proto_spikes[:, 4])[0],
                np.where(proto_spikes[:, 3])[0],
                np.where(proto_spikes[:, 2])[0],
                np.where(proto_spikes[:, 1])[0],
                np.where(proto_spikes[:, 0])[0],
                np.where(error_spikes)[0],
                np.where(novelty_spikes)[0],
                np.where(s_pattern_inp[0, :])[0]],
        time_series=preds,
        y_label_time_series = &#39;Predictions&#39;,
        time=time,
        figsize=(20, 8),
        legend=[&#39;Prototype Neuron 9&#39;, &#39;Prototype Neuron 8&#39;,
                &#39;Prototype Neuron 7&#39;, &#39;Prototype Neuron 6&#39;,
                &#39;Prototype Neuron 5&#39;, &#39;Prototype Neuron 4&#39;,
                &#39;Prototype Neuron 3&#39;, &#39;Prototype Neuron 2&#39;,
                &#39;Prototype Neuron 1&#39;, &#39;Prototype Neuron 0&#39;,
                &#39;Error Spikes&#39;, &#39;Novelty Spikes&#39;, &#39;Input Injection&#39;],
        colors=[&#39;blue&#39;, &#39;#458b00&#39;, &#39;#f14a16&#39;, &#39;#458b00&#39;, &#39;#ff9912&#39;, &#39;#458b00&#39;, &#39;purple&#39;, &#39;#f14a16&#39;, &#39;blue&#39;, &#39;#ff9912&#39;,&#39;r&#39;,&#39;#458b00&#39;, &#39;#f14a16&#39;, &#39;#458b00&#39;],
        title=&#39;Spike Raster Plot&#39;,
        num_steps=num_steps
)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_32_0.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_32_0.png" />
</div>
</div>
</section>
<section id="Testing-phase">
<h4>Testing phase<a class="headerlink" href="#Testing-phase" title="Permalink to this heading"></a></h4>
<p>We turn off both novelty and error-triggered allocation mechanisms to evaluate the model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>clp.supervised = False
clp.learn_novels = False

s_pattern_inp, s_user_label = clp.generate_input_spikes(X_test)
clp = clp.setup_procs_and_conns()

num_steps = clp.num_steps

# Run
run_cond = RunSteps(num_steps=num_steps)

clp.prototypes.run(condition=run_cond, run_cfg=run_cfg)

novelty_spikes, proto_spikes, error_spikes, preds,currs = clp.get_results()
<br/></pre></div>
</div>
</div>
</section>
</section>
</section>
<section id="Performance-results">
<h2>Performance results<a class="headerlink" href="#Performance-results" title="Permalink to this heading"></a></h2>
<p>Below, we calculate the accuracy of the test set (26 frames per object) after the learning is finished. As expected, the results are near to offline K-NN results. We also break down the error into two components: confusion between the learned classes and false positive novelty detections (i.e., not responding to some instances). One can observe that the confusion rate is precisely the same as offline KNN results. However, because the prototype in CLP has detection thresholds, they may
misclassify some known instances as unknowns. This can be solved by tuning the hyperparameters like voltage threshold, however, we need to hit a balance between</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_pred = np.maximum.reduceat(preds.copy(), np.r_[0:len(preds):n_steps_per_sample])[:,0]
novelty_pred = np.maximum.reduceat(novelty_spikes.copy(), np.r_[0:len(novelty_spikes):n_steps_per_sample])[:,0]

acc = accuracy_score(y_test, y_pred).round(3)
print(&quot;Test accuracy on offline trained classes (including no response cases): &quot;, acc)

err_rate = (np.sum(y_pred[y_pred!=0] != y_test[y_pred!=0])/len(y_test)).round(3)
print(&quot;Error rate on test set (excluding no response cases): &quot;, err_rate)

y_known = np.zeros(shape=y_test.shape)
nvl_FP = (1-accuracy_score(y_known, novelty_pred)).round(3)
print(&quot;False positive rate for novelty detection: &quot;, nvl_FP)

cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6,6))
plt.xticks(np.arange(cls_rng[1]+1))
plt.yticks(np.arange(cls_rng[1]+1))
plt.imshow(cm)
plt.colorbar()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Test accuracy on offline trained classes (including no response cases):  0.894
Error rate on test set (excluding no response cases):  0.038
False positive rate for novelty detection:  0.067
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_36_1.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_36_1.png" />
</div>
</div>
<section id="Experiment-2:-Novelty-detection-of-unknown-classes">
<h3>Experiment 2: Novelty detection of unknown classes<a class="headerlink" href="#Experiment-2:-Novelty-detection-of-unknown-classes" title="Permalink to this heading"></a></h3>
<p>In this experiment, we will evaluate the novelty detection capability on unknown classes and see how many per cent of the frames from the ten unknown classes can be detected as a novelty after we have learnt the first 20 classes above.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>## OOD detection data prep
cls_rng = [20, 30]
n_classes = cls_rng[1]

X_ood = X_sampled[(y_sampled&gt;cls_rng[0]) &amp; (y_sampled&lt;=cls_rng[1]), :]
y_ood = y_sampled[(y_sampled&gt;cls_rng[0]) &amp; (y_sampled&lt;=cls_rng[1])]

X_ood = X_ood[np.argsort(y_ood),:]
y_ood = y_ood[np.argsort(y_ood)]

print(X_ood.shape)
print(y_ood.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(360, 1280)
(360,)
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>clp.supervised = False
clp.learn_novels = False

s_pattern_inp, s_user_label = clp.generate_input_spikes(X_ood)
clp = clp.setup_procs_and_conns()

num_steps = clp.num_steps

# Run
run_cond = RunSteps(num_steps=num_steps)

clp.prototypes.run(condition=run_cond, run_cfg=run_cfg)

novelty_spikes, proto_spikes, error_spikes, preds, _ = clp.get_results()
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>novelty_pred = np.maximum.reduceat(novelty_spikes.copy(), np.r_[0:len(novelty_spikes):n_steps_per_sample])

y_unknown = np.ones(shape=y_ood.shape)
print(&quot;Correct novelty detection rate: &quot;, accuracy_score(y_unknown, novelty_pred).round(3))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Correct novelty detection rate:  0.681
</pre></div></div>
</div>
</section>
<section id="Experiment-3:-Few-shot-open-set-unsupervised-OCL">
<h3>Experiment 3: Few-shot open-set unsupervised OCL<a class="headerlink" href="#Experiment-3:-Few-shot-open-set-unsupervised-OCL" title="Permalink to this heading"></a></h3>
<p>This experiment will test learning novel classes on top of base classes. We simulate real-world dynamic learning by presenting the model with familiar and novel classes. The error-based allocation mechanism is turned off: the learning happens only through novelty detection triggered allocation, hence unsupervised. The prototypes allocated via this mechanism receive a temporary pseudo-label, which can be updated later with actual labels. We provide labels for all training examples to simulate
this kind of post-labelling. However, they do not trigger learning themselves but are used to label newly allocated prototypes just after their allocation. For each object (class), 1, 5 or 10 frames are provided for training to test few-shot learning performance. The test set includes all 20 base classes and 10 novel classes to measure accuracy for both groups of classes.</p>
<section id="Data-preperation">
<h4>Data preperation<a class="headerlink" href="#Data-preperation" title="Permalink to this heading"></a></h4>
<p>We reserve 10 frames for training, however we will use 1, 5 or all of them to train model in different versions of the few-shot learning experiment.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>test_size = 26/36
X_train_on, X_test_on, y_train_on, y_test_on = train_test_split(X_sampled, y_sampled, test_size=test_size, random_state=1000, stratify=y_sampled)

X_train_on = X_train_on[np.argsort(y_train_on),:]
y_train_on = y_train_on[np.argsort(y_train_on)]
X_test_on = X_test_on[np.argsort(y_test_on),:]
y_test_on = y_test_on[np.argsort(y_test_on)]

cls_rng = [20, 30]
n_classes = cls_rng[1]

X_train_on = X_train_on[(y_train_on&gt;cls_rng[0]) &amp; (y_train_on&lt;=cls_rng[1]), :]
X_test_on = X_test_on[(y_test_on&gt;cls_rng[0]) &amp; (y_test_on&lt;=cls_rng[1]), :]
y_train_on = y_train_on[(y_train_on&gt;cls_rng[0]) &amp; (y_train_on&lt;=cls_rng[1])]
y_test_on = y_test_on[(y_test_on&gt;cls_rng[0]) &amp; (y_test_on&lt;=cls_rng[1])]

print(X_train_on.shape)
print(X_test_on.shape)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(100, 1280)
(260, 1280)
</pre></div></div>
</div>
</section>
<section id="Save-the-model">
<h4>Save the model<a class="headerlink" href="#Save-the-model" title="Permalink to this heading"></a></h4>
<p>The model is saved by storing learned prototypes and their labels because we will do multiple experiments on the same base model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>proto_labels = clp.readout_layer.proto_labels.get()
weights_proto = clp.dense_proto.weights.get()
clp.prototypes.stop()
</pre></div>
</div>
</div>
</section>
<section id="Train-and-test">
<h4>Train and test<a class="headerlink" href="#Train-and-test" title="Permalink to this heading"></a></h4>
<p>The main training and testing loop for <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">[1,5,10]</span></code> of three experiments. <code class="docutils literal notranslate"><span class="pre">supervised</span></code> parameter is turned off to make learning the novel ten classes purely unsupervised. For testing, <code class="docutils literal notranslate"><span class="pre">learn_novels</span></code> is turned off too. For all three experiments, the same saved base model is used.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

old_class_accs = []
inc_class_accs = []
total_accs = []

k_shot_list = [1, 5, 10]

for k_shot in k_shot_list:

    X_train_k_shot = np.reshape(X_train_on, (-1, 10, X_train_on.shape[-1]))[:, :k_shot].reshape((-1, X_train_on.shape[-1]))
    X_test_k_shot = np.reshape(X_test_on, (-1, 10, X_test_on.shape[-1]))[:, :k_shot].reshape((-1, X_test_on.shape[-1]))
    y_train_k_shot = np.reshape(y_train_on, (-1, 10))[:, :k_shot].reshape((-1,))
    y_test_k_shot = np.reshape(y_test_on, (-1, 10,))[:, :k_shot].reshape((-1,))

    # #------------- Incremental training ------------#

    clp = CLP(supervised=False,
              learn_novels=True,
              weights_proto=weights_proto,
              proto_labels=proto_labels,
              n_protos=n_protos,
              n_features=n_features,
              n_steps_per_sample=n_steps_per_sample,
              b_fraction=b_fraction,
              du=du,
              dv=dv,
              vth=vth,
              t_wait=t_wait)
    s_pattern_inp, s_user_label = clp.generate_input_spikes(X_train_k_shot, y_train_k_shot)
    clp = clp.setup_procs_and_conns()

    num_steps = clp.num_steps

    # Run
    run_cond = RunSteps(num_steps=num_steps)

    clp.prototypes.run(condition=run_cond, run_cfg=run_cfg)

    #---------------- Testing -----------------------#

    clp.supervised = False
    clp.learn_novels = False

    s_pattern_inp, s_user_label = clp.generate_input_spikes(np.vstack((X_test, X_test_on)))
    clp = clp.setup_procs_and_conns()

    num_steps = clp.num_steps

    # Run
    run_cond = RunSteps(num_steps=num_steps)
    clp.prototypes.run(condition=run_cond, run_cfg=run_cfg)

    novelty_spikes, proto_spikes, error_spikes, preds, currs = clp.get_results()
    y_pred = np.maximum.reduceat(preds.copy(), np.r_[0:len(preds):n_steps_per_sample])

    old_cls_acc = accuracy_score(y_test, y_pred[:len(y_test)])
    new_cls_acc = accuracy_score(y_test_on, y_pred[len(y_test):])

    y_all = np.concatenate((y_test,y_test_on))
    total_acc = accuracy_score(y_all, y_pred)

    print(&quot;k = &quot; ,k_shot)
    print(&quot;Accuracy for old classes: &quot;, old_cls_acc.round(3))
    print(&quot;Accuracy for new classes: &quot;, new_cls_acc.round(3))
    print(&quot;Total accuracy: &quot;, total_acc.round(3))
    print(&quot;--------------------------------&quot;)

    old_class_accs.append(old_cls_acc)
    inc_class_accs.append(new_cls_acc)
    total_accs.append(total_acc)

    clp.prototypes.stop()

old_class_accs = np.array(old_class_accs)
inc_class_accs = np.array(inc_class_accs)
total_accs = np.array(total_accs)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
k =  1
Accuracy for old classes:  0.894
Accuracy for new classes:  0.404
Total accuracy:  0.731
--------------------------------
k =  5
Accuracy for old classes:  0.894
Accuracy for new classes:  0.738
Total accuracy:  0.842
--------------------------------
k =  10
Accuracy for old classes:  0.894
Accuracy for new classes:  0.754
Total accuracy:  0.847
--------------------------------
</pre></div></div>
</div>
</section>
<section id="Visualize-the-results">
<h4>Visualize the results<a class="headerlink" href="#Visualize-the-results" title="Permalink to this heading"></a></h4>
<p>Below we visualize the results from the experiment. Three types of accuracies (for base classes, novel classes and combined) are plotted for all three values of k (1, 5 and 10-shot). The following observation can be made from the results: - Accuracy of the 20 base classes does not drop after learning ten novel classes, i.e., no forgetting occurred. - Unsupervised 1-shot and 5-shot learning accuracies of novel classes are compelling but does not reach supervised multi-shot learning levels - More
shots, i.e., more frames for training each object, helps to increase this accuracy almost to the supervised levels.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[57]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.figure(figsize=(8,4))
X_axis = np.arange(3)
plt.bar(X_axis-0.2, old_class_accs, 0.2, label=&#39;Base classes (10-shot, supervised)&#39;)
plt.bar(X_axis, inc_class_accs, 0.2, label=&#39;Novel classes (k-shot, unsupervised)&#39;)
plt.bar(X_axis+0.2, total_accs, 0.2, label=&#39;Total accuracy&#39;)

plt.ylim([0,1.3])
plt.xticks(X_axis, k_shot_list)
plt.yticks(np.arange(0.1,1.1,0.1))
plt.grid(linewidth=0.5, axis=&#39;y&#39;)

plt.title(&quot;Few-shot unsupervised OCL with 20 base and 10 incremental classes&quot;)
plt.xlabel(&quot;k: number of frames per class used in incremental learning&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_49_0.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_49_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[58]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>y_pred = np.maximum.reduceat(preds.copy(), np.r_[0:len(preds):n_steps_per_sample])

print(&quot;Old classes&#39; accuracy: &quot;, accuracy_score(y_test, y_pred[:len(y_test)]))
print(&quot;New classes&#39; accuracy: &quot;, accuracy_score(y_test_on, y_pred[len(y_test):]))
y_all = np.concatenate((y_test,y_test_on))
acc = accuracy_score(y_all, y_pred)
print(&quot;Accuracy: &quot;, acc)

cm = confusion_matrix(y_all, y_pred)
plt.figure(figsize=(5,5))
plt.xticks(np.arange(n_classes+1))
plt.yticks(np.arange(n_classes+1))
plt.imshow(cm)
plt.colorbar()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Old classes&#39; accuracy:  0.8942307692307693
New classes&#39; accuracy:  0.7538461538461538
Accuracy:  0.8474358974358974
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_50_1.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_50_1.png" />
</div>
</div>
</section>
</section>
<section id="Experiment-4:-Few-shot-supervised-OCL">
<h3>Experiment 4: Few-shot supervised OCL<a class="headerlink" href="#Experiment-4:-Few-shot-supervised-OCL" title="Permalink to this heading"></a></h3>
<p>This experiment will extend Experiment 1 into a few-shot learning regime. Like Experiment 3, the model will learn ten novel classes on top of the base model that already knows the 20 objects. However, this time, we will provide labels for training: every time a wrong prediction is made, the error signal triggers allocation, hence learning. Note that the novelty detection triggered learning is still in place with the labelling mechanism.</p>
<section id="Train-and-Test">
<h4>Train and Test<a class="headerlink" href="#Train-and-Test" title="Permalink to this heading"></a></h4>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

old_class_accs = []
inc_class_accs = []
total_accs = []

k_shot_list = [1, 5, 10]

for k_shot in k_shot_list:

    X_train_k_shot = np.reshape(X_train_on, (-1, 10, X_train_on.shape[-1]))[:, :k_shot].reshape((-1, X_train_on.shape[-1]))
    X_test_k_shot = np.reshape(X_test_on, (-1, 10, X_test_on.shape[-1]))[:, :k_shot].reshape((-1, X_test_on.shape[-1]))
    y_train_k_shot = np.reshape(y_train_on, (-1, 10))[:, :k_shot].reshape((-1,))
    y_test_k_shot = np.reshape(y_test_on, (-1, 10,))[:, :k_shot].reshape((-1,))

    # #------------- Incremental training ------------#

    clp = CLP(supervised=True,
              learn_novels=True,
              weights_proto=weights_proto,
              proto_labels=proto_labels,
              n_protos=n_protos,
              n_features=n_features,
              n_steps_per_sample=n_steps_per_sample,
              b_fraction=b_fraction,
              du=du,
              dv=dv,
              vth=vth,
              t_wait=t_wait)
    s_pattern_inp, s_user_label = clp.generate_input_spikes(X_train_k_shot, y_train_k_shot)
    clp = clp.setup_procs_and_conns()

    num_steps = clp.num_steps

    # Run
    run_cond = RunSteps(num_steps=num_steps)

    clp.prototypes.run(condition=run_cond, run_cfg=run_cfg)

    #---------------- Testing -----------------------#

    clp.supervised = False
    clp.learn_novels = False

    s_pattern_inp, s_user_label = clp.generate_input_spikes(np.vstack((X_test, X_test_on)))
    clp = clp.setup_procs_and_conns()

    num_steps = clp.num_steps

    # Run
    run_cond = RunSteps(num_steps=num_steps)
    clp.prototypes.run(condition=run_cond, run_cfg=run_cfg)

    novelty_spikes, proto_spikes, error_spikes, preds, currs = clp.get_results()
    y_pred = np.maximum.reduceat(preds.copy(), np.r_[0:len(preds):n_steps_per_sample])

    old_cls_acc = accuracy_score(y_test, y_pred[:len(y_test)])
    new_cls_acc = accuracy_score(y_test_on, y_pred[len(y_test):])

    y_all = np.concatenate((y_test,y_test_on))
    total_acc = accuracy_score(y_all, y_pred)

    print(&quot;k = &quot; ,k_shot)
    print(&quot;Accuracy for old classes: &quot;, old_cls_acc.round(3))
    print(&quot;Accuracy for new classes: &quot;, new_cls_acc.round(3))
    print(&quot;Total accuracy: &quot;, total_acc.round(3))
    print(&quot;--------------------------------&quot;)

    old_class_accs.append(old_cls_acc)
    inc_class_accs.append(new_cls_acc)
    total_accs.append(total_acc)

    clp.prototypes.stop()

old_class_accs = np.array(old_class_accs)
inc_class_accs = np.array(inc_class_accs)
total_accs = np.array(total_accs)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
k =  1
Accuracy for old classes:  0.894
Accuracy for new classes:  0.5
Total accuracy:  0.763
--------------------------------
k =  5
Accuracy for old classes:  0.894
Accuracy for new classes:  0.885
Total accuracy:  0.891
--------------------------------
k =  10
Accuracy for old classes:  0.894
Accuracy for new classes:  0.915
Total accuracy:  0.901
--------------------------------
</pre></div></div>
</div>
</section>
<section id="id2">
<h4>Visualize the results<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h4>
<p>Similar to Experiment 3, we visualize the results in a bar plot. Subsequently, we make the following observations: - Still no forgetting of old knowledge (blue), even when labels are used during training. - Regarding 1-shot learning, supervision does not improve the accuracy of novel classes much compared to the unsupervised version. - However, with 5-shot and supervision, the performance improves significantly: it reaches similar levels to the base classes and surpasses the unsupervised
version, as expected. - Adding five more frames on top of this (i.e. 10-shot) does not bring much extra improvement.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.figure(figsize=(8,4))
X_axis = np.arange(3)
plt.bar(X_axis-0.2, old_class_accs, 0.2, label=&#39;Base classes (10-shot, supervised)&#39;)
plt.bar(X_axis, inc_class_accs, 0.2, label=&#39;Novel classes (k-shot, supervised)&#39;)
plt.bar(X_axis+0.2, total_accs, 0.2, label=&#39;Total accuracy&#39;)

plt.ylim([0,1.3])
plt.xticks(X_axis, k_shot_list)
plt.yticks(np.arange(0.1,1.1,0.1))
plt.grid(linewidth=0.5, axis=&#39;y&#39;)

plt.title(&quot;Few-shot supervised OCL with 20 base and 10 incremental classes&quot;)
plt.xlabel(&quot;k: number of frames per class used in incremental learning&quot;)
plt.ylabel(&quot;Accuracy&quot;)
plt.legend()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_55_0.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_55_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score

y_pred = np.maximum.reduceat(preds.copy(), np.r_[0:len(preds):n_steps_per_sample])

print(&quot;Old classes&#39; accuracy: &quot;, accuracy_score(y_test, y_pred[:len(y_test)]))
print(&quot;New classes&#39; accuracy: &quot;, accuracy_score(y_test_on, y_pred[len(y_test):]))
y_all = np.concatenate((y_test,y_test_on))
acc = accuracy_score(y_all, y_pred)
print(&quot;Accuracy: &quot;, acc)

cm = confusion_matrix(y_all, y_pred)
plt.figure(figsize=(5,5))
plt.xticks(np.arange(n_classes+1))
plt.yticks(np.arange(n_classes+1))
plt.imshow(cm)
plt.colorbar()
plt.show()
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Old classes&#39; accuracy:  0.8942307692307693
New classes&#39; accuracy:  0.9153846153846154
Accuracy:  0.9012820512820513
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_56_1.png" src="../../../../_images/lava_notebooks_in_depth_clp_tutorial02_clp_on_coil100_56_1.png" />
</div>
</div>
</section>
</section>
<section id="How-to-learn-more?">
<h3>How to learn more?<a class="headerlink" href="#How-to-learn-more?" title="Permalink to this heading"></a></h3>
<p>Learn about details of CLP algorithms and its main processes in the <a class="reference internal" href="tutorial01_one-shot_learning_with_novelty_detection.html"><span class="doc">introductory in-depth tutorial on CLP</span></a>.</p>
<p>If you want to find out more about CLP, have a look at the <a class="reference external" href="https://lava-nc.org/">Lava documentation</a> or dive into the <a class="reference external" href="https://github.com/lava-nc/lava/tree/main/src/lava/proc/clp">source code</a>.</p>
<p>To receive regular updates on the latest developments and releases of the Lava Software Framework please subscribe to the <a class="reference external" href="http://eepurl.com/hJCyhb">INRC newsletter</a>.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2021, Intel Corporation.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(false);
      });
  </script> 

</body>
</html>